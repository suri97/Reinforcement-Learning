{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from vizdoom import *\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env():\n",
    "    game = DoomGame()\n",
    "    game.load_config('basic.cfg')\n",
    "    game.set_doom_scenario_path('basic.wad')\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    left = [1,0,0]\n",
    "    right = [0,1,0]\n",
    "    shoot = [0,0,1]\n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_env()\n",
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    \n",
    "    cropped_frame = frame[220:-100, 100:-100]\n",
    "    #plt.imshow(cropped_frame, cmap='gray')\n",
    "    #plt.show()\n",
    "    #exit()\n",
    "    \n",
    "    normalised_frame = cropped_frame/255.0\n",
    "    \n",
    "    preprocessed_frame = cv2.resize( normalised_frame, (84,84) )\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "\n",
    "stacked_frames = deque( [ np.zeros( (84,84) , dtype = np.int ) for i in range(stack_size) ], maxlen = 4 )\n",
    "\n",
    "def stack_frames( stacked_frames, state, is_new_episode ):\n",
    "    \n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        \n",
    "        stacked_frames = deque( [np.zeros( (84,84) , dtype = np.int ) for i in range(stack_size) ], maxlen = 4 )\n",
    "        \n",
    "        for _ in range(4):\n",
    "            stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack( stacked_frames, axis=2 )\n",
    "    \n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack( stacked_frames, axis = 2 )\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [84,84,4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 500        # Total episodes for training\n",
    "max_steps = 100              # Max possible steps in an episode\n",
    "batch_size = 64             \n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.95               # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = False\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            \n",
    "            self.inputs = tf.placeholder( tf.float32, [None, 84,84,4], name='inputs' )\n",
    "            self.actions_ = tf.placeholder( tf.float32, [None,3], name = 'actions_' )\n",
    "            \n",
    "            self.target_Q = tf.placeholder( tf.float32, [None], name = 'target' )\n",
    "            \n",
    "            ## First Conv Layer\n",
    "            self.conv1 = tf.layers.conv2d( inputs=self.inputs, filters=32, kernel_size=[8,8], strides=[4,4], padding='VALID', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='conv1' )\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1, epsilon=1e-5, training=True, name = 'batch_norm1' )\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu( self.conv1_batchnorm, name = 'conv1_out' )\n",
    "            \n",
    "            ## Second Conv layer\n",
    "            self.conv2 = tf.layers.conv2d( inputs=self.conv1_out, filters=64, kernel_size=[4,4], strides=[2,2], padding='VALID', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='conv2' )\n",
    "            \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2, epsilon=1e-5, training=True, name = 'batch_norm2' )\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu( self.conv2_batchnorm, name = 'conv2_out' )\n",
    "            \n",
    "            ## Third Convolution\n",
    "            self.conv3 = tf.layers.conv2d( inputs=self.conv2_out, filters=128, kernel_size=[4,4], strides=[2,2], padding='VALID', kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(), name='conv3' )\n",
    "            \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3, epsilon=1e-5, training=True, name = 'batch_norm3' )\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu( self.conv3_batchnorm, name = 'conv3_out' )\n",
    "            \n",
    "            ## Flatten\n",
    "            self.flatten = tf.layers.flatten( self.conv3_out )\n",
    "            \n",
    "            self.fc = tf.layers.dense( inputs= self.flatten, units=512, activation=tf.nn.relu, name = 'fc1', kernel_initializer=tf.contrib.layers.xavier_initializer() )\n",
    "            \n",
    "            self.output = tf.layers.dense( inputs= self.fc, units=3, kernel_initializer=tf.contrib.layers.xavier_initializer(), activation=None )\n",
    "            \n",
    "            self.Q = tf.reduce_sum( tf.multiply( self.output, self.actions_ ), axis=1 )\n",
    "            \n",
    "            self.loss = tf.reduce_mean( tf.square( self.target_Q - self.Q ) )\n",
    "            self.optimiser = tf.train.RMSPropOptimizer( self.learning_rate ).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "DQNetwork = DQNetwork( state_size, action_size, learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
      "('Score: ', 51.0)\n",
      "('Score: ', 95.0)\n",
      "('Score: ', 32.0)\n",
      "('Score: ', 33.0)\n",
      "('Score: ', 17.0)\n",
      "('TOTAL_SCORE', 2.28)\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    game, possible_actions = create_env()\n",
    "    \n",
    "    totalScore = 0\n",
    "    \n",
    "   \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./model/model.ckpt\")\n",
    "    game.init()\n",
    "    for i in range(5):\n",
    "        \n",
    "        game.new_episode()\n",
    "        start = True\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            frame = game.get_state().screen_buffer\n",
    "            state, _ = stack_frames(stacked_frames, frame, start)\n",
    "            #break\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs: state.reshape((1, 84,84,4))})\n",
    "            action = np.argmax(Qs)\n",
    "            action = possible_actions[int(action)]\n",
    "            game.make_action(action)        \n",
    "            score = game.get_total_reward()\n",
    "            start = False\n",
    "            \n",
    "        print(\"Score: \", score)\n",
    "        totalScore += score\n",
    "    print(\"TOTAL_SCORE\", totalScore/100.0)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
